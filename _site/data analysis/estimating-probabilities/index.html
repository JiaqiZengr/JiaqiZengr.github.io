<!DOCTYPE html>
<html>

  <head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Estimating Probabilities - Jiaqi Zeng</title>

	<link rel="shortcut icon" href="/styles/images/favicon.jpg">
	<link rel="icon" href="/styles/images/favicon.jpg">

	<link rel="stylesheet" href="/styles/css/index.css">
	<link rel="stylesheet" href="/styles/css/fontawesome/css/font-awesome.min.css">
	<link rel="stylesheet" href="/styles/css/syntax.css">
	<link rel="canonical" href="/data%20analysis/estimating-probabilities/">
	<link rel="alternate" type="application/rss+xml" title="Jiaqi Zeng" href="/feed.xml">
	
	<meta name="keywords" content="Estimating Probabilities, Jiaqi Zeng, 曾佳琪;个人博客">
	<meta name="description" content="曾佳琪;个人博客">

	<script src="/styles/js/jquery.min.js"></script>
	<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->
	<!-- <script src="https://cdn.bootcss.com/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->
	<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [['$','$'], ['\\(','\\)']],
			processEscapes: true
		}
		});
		</script>
	<!--[if lt IE 9]>
    	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  	<![endif]-->
  	<style type="text/css">
	  	.docs-content{
	  		margin-bottom: 10px;
	  	}
  	</style>
</head>

  <body class="index">

    <header class="navbar navbar-inverse navbar-fixed-top docs-nav" role="banner">
  <div class="container">
    <div class="navbar-header">
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target=".bs-navbar-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a href="/" class="navbar-brand">
        Gaby
      </a>
    </div>
    <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation">
      <ul class="nav navbar-nav">    
        <li>
          <a href="/">Home</a>
        </li>
        <li>
          <a href="/categories/">Categories</a>
        </li>
        <li>
          <a href="/tag">Tags</a>
        </li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown">About<b class="caret"></b></a>
          <ul class="dropdown-menu">
            <li><a rel="nofollow" target="_blank" href="https://github.com/JiaqiZengr/">My Github</a></li>
          </ul>
        </li>
      </ul>
    </nav>
  </div>
</header>
    <div class="docs-header" id="content">
  <div class="container">
  	
  		<!--
		    <h1>Estimating Probabilities</h1>
		    <p>Post on Mar 08, 2019 by <a href="/about">Jiaqi Zeng</a></p>
		-->
		    <h1>Best wishes</h1>
    
  </div>
</div>
    
      
<div class="banner">
  <div class="container">
  	
    	<a href="/categories/#Data Analysis-ref">Data Analysis</a>	/
    	<a href="/tag/#Estimating Probabilities-ref">Estimating Probabilities</a>
    
  </div>
</div>

    

    <div class="container docs-container">
  <div class="row">
    <div class="col-md-3">
      <div class="sidebar hidden-print" role="complementary">
        <div id="navigation">
  <h1>Contents</h1>
  <ul class="nav sidenav">
  </ul>
</div>

 
      </div>
    </div>
    <div class="col-md-9" role="main">
      <div class="panel docs-content">
        <div class="wrapper">
            <header class="post-header">
              <h1 class="post-title">Estimating Probabilities</h1>
              <!--
                <p class="post-meta">Mar 8, 2019</p>
              -->
              <div class="meta">Posted on <span class="postdate">March 8, 2019</span> By <a target="_blank" href="https://JiaqiZengr.github.io">Jiaqi Zeng</a></div>
              <br />
            </header>
            <article class="post-content">
              <ol id="markdown-toc">
  <li><a href="#definition-of-mle-and-map" id="markdown-toc-definition-of-mle-and-map">Definition of MLE and MAP</a></li>
  <li><a href="#an-example-of-coin-flips" id="markdown-toc-an-example-of-coin-flips">An example of coin flips</a></li>
  <li><a href="#inference" id="markdown-toc-inference">Inference</a></li>
  <li><a href="#more-about-map-and-mle" id="markdown-toc-more-about-map-and-mle">More about MAP and MLE</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ol>

<p>Estimating probabilities is important in statistical research. Today we will talk about two most common approaches to estimate probabilities: maximum likelihood estimation (MLE) and maximum a posteriori estimation (MAP).</p>

<h2 id="definition-of-mle-and-map">Definition of MLE and MAP</h2>

<ul>
  <li>Maximum likelihood estimation (MLE)’s principle is that if we observe training data $D$,  we should choose the value of $\theta$ that makes $D$ most probable.</li>
</ul>

<script type="math/tex; mode=display">\hat{\theta}^{MLE} = \arg\max_\theta{P(D|\theta)}</script>

<ul>
  <li>Maximium a posteriori probability (MAP) estimation is to choose the most probable value of $\theta$, given the observed training data plus a prior probability distribution $P(\theta)$ which captures prior knowledge or assumptions about the value of $\theta$.</li>
</ul>

<script type="math/tex; mode=display">\hat{\theta}^{MAP} = \arg\max_\theta{P(\theta|D)}</script>

<h2 id="an-example-of-coin-flips">An example of coin flips</h2>

<p>Imagine we flip coins many times, let $X = 1$ if it turns up heads and $X=0$ if it turns up tails. We should estimate the probability that it will turn up heads, that is, to estimate $P(X = 1)$.</p>

<p>We flip the coin n times and observe that it turns up heads $\alpha_0$ times, and tails $\alpha_1$ times. $n =\alpha_0 + \alpha_1$.</p>

<p>There are two assumptions:</p>

<ol>
  <li>
    <p>The outcomes of the flips are independent. That means the result of one coin flip has no influence on other coin flips.</p>
  </li>
  <li>
    <p>The outcomes are identically distributed. That means each coin flip has the same value of $\theta$.</p>
  </li>
</ol>

<h2 id="inference">Inference</h2>

<p>The maximum likelihood (MLE) principle involves choosing $\theta$ to maximize $P(D|\theta)$. We can write formula as follow:</p>

<script type="math/tex; mode=display">P(D=\langle{\alpha_1,\alpha_0}\rangle|\theta) = \theta^{\alpha_1}(1-\theta)^{\alpha_0}</script>

<p>This likelihood function is often written $L(\theta) = P(D|\theta)$. Notice that maximizing $P(D|\theta)$ with respect to $\theta$ is equivalent to maximizing its logarithm, $\ln P(D|\theta) $ with respect to $\theta$, because $\ln x$ increases monotonically with $x$. So we write $l(\theta) = \ln P(D|\theta)$ and find $\theta$ which maxmizes $l(\theta)$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
\frac{\partial{l(\theta)}}{\partial{\theta}} &= \frac{\partial\ln{P(D|\theta)}}{\partial{\theta}} \\
&= \frac{\partial\ln{[\theta^{\alpha_1}(1-\theta)^{\alpha_0}]}}{\partial{\theta}} \\
&= \frac{\partial[\alpha_1\ln{\theta}+\alpha_0\ln(1-\theta)]}{\partial{\theta}} \\
&= \alpha_1\frac{\partial\ln\theta}{\partial\theta} + \alpha_0\frac{\partial\ln(1-\theta)}{\partial\theta} \\ 
&= \alpha_1\frac{1}{\theta} - \alpha_0\frac{1}{1-\theta}
\end{split}
\end{equation} %]]></script>

<p>We let it equals to zero and simplify it. We can get:</p>

<script type="math/tex; mode=display">\hat{\theta} = \frac{\alpha_1}{\alpha_1+\alpha_0}</script>

<p>Also, given observed training data producing $\alpha_1$ observed “heads”, and $\alpha_2$ observed ”tails”, plus prior information expressed by introducing  imaginary “heads” and  imaginary “tails”, output the estimate:</p>

<script type="math/tex; mode=display">\hat{\theta} = \frac{\alpha_1+\gamma_1}{(\alpha_1+\gamma_1)+(\alpha_0+\gamma_0)}</script>

<h2 id="more-about-map-and-mle">More about MAP and MLE</h2>

<p>What we should know is that MAP needs background assumptions of its value which MLE doesn’t need. In other words, MAP assumes background knowledge is available.</p>

<p>If MAP priors are correct, then MAP is a better estimation than MLE. But if MAP priors are incorrect, MLE is better.</p>

<p>Follow pictures assume $\theta = 0.3$ and picture 1 has correct MAP priors but picture doesn’t, the blue line if estimation of MLE and the red line is estimation of MAP. We can discover that MAP is closer to real $\theta$ when MAP has correct priors. On the contrary, MLE is closer to real $\theta$ when MAP priors are wrong. Also, as the size of the data grows, the MLE and MAP estimates converge toward each other, and toward the correct estimate for $\theta$.</p>

<p><img src="/styles/images/estimating-probabilities/8.png" alt="img1" /></p>

<h2 id="reference">Reference</h2>

<ol>
  <li>Mitchell, ESTIMATING PROBABILITIES: MLE AND MAP</li>
</ol>

            </article>
        </div>
      </div>
    </div>
  </div>
</div>

    
    <footer class="footer" role="contentinfo">
	<div class="container">
		<p class="copyright">Copyright &copy; 2018-2019 <a href="https://JiaqiZengr.github.io">Jiaqi Zeng</a>.</p>
		<p>Powered by <a href="http://jekyllrb.com">Jekyll</a></p>
	</div>
</footer>

<script src="/styles/js/jquery.min.js"></script>
<script src="/styles/js/bootstrap.min.js"></script>
<script src="/styles/js/holder.min.js"></script>
<script src="/styles/js/lessismore.js"></script>
<script src="/styles/js/application.js"></script>

  </body>
</html>
